import os
import random
from typing import List, Tuple

import torch
from torch.utils.data import Dataset, DataLoader, random_split

from src.data.tokenizer import Tokenizer
from src.config.model_config import Config

# ---------------------------
# Helper: load vocab into tokenizer
# ---------------------------
def load_vocab_to_tokenizer(tokenizer: Tokenizer, vocab_path: str):
    tokenizer.vocab = {}
    with open(vocab_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split("\t")
            if len(parts) == 2:
                word, idx = parts
            else:
                # fallback for other formats (word idx)
                word, idx = parts[0], parts[-1]
            tokenizer.vocab[word] = int(idx)
    return tokenizer

# ---------------------------
# Build long token list from corpus
# ---------------------------
def corpus_to_token_ids(tokenizer: Tokenizer, corpus_path: str) -> List[int]:
    token_ids = []
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # simple whitespace tokenization (consistent with your Tokenizer)
            words = line.lower().split()
            ids = [tokenizer.vocab[w] for w in words if w in tokenizer.vocab]
            token_ids.extend(ids)
    return token_ids

# ---------------------------
# Dataset: sliding-window sequence dataset for LM next-token prediction
# ---------------------------
class LMSequenceDataset(Dataset):
    """
    Produces (input_ids, target_ids) pairs.
    input_ids: seq_len tokens
    target_ids: seq_len tokens (the next-token shifted sequence)
    Generated by sliding window over a long token list.
    """
    def __init__(self, token_ids: List[int], seq_len: int = 32, stride: int = 1):
        assert len(token_ids) > seq_len, "Corpus too small for the given seq_len"
        self.seq_len = seq_len
        self.stride = stride
        self.token_ids = token_ids

        # compute start indices for sliding windows
        self.starts = list(range(0, len(token_ids) - seq_len, stride))

    def __len__(self):
        return len(self.starts)

    def __getitem__(self, idx):
        start = self.starts[idx]
        inp = self.token_ids[start : start + self.seq_len]
        tgt = self.token_ids[start + 1 : start + self.seq_len + 1]
        return torch.tensor(inp, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)

# ---------------------------
# Collate function (not strictly necessary for fixed-length sequences,
# but keeps interface consistent and allows padding in the future)
# ---------------------------
def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]]):
    # batch is list of tuples (inp, tgt), each shape [seq_len]
    inputs = torch.stack([item[0] for item in batch], dim=0)
    targets = torch.stack([item[1] for item in batch], dim=0)
    return inputs, targets

# ---------------------------
# Create dataloaders (train / val split)
# ---------------------------
def create_dataloaders_from_corpus(
    corpus_path: str = Config.DATA_PATH,
    vocab_path: str = Config.VOCAB_PATH,
    seq_len: int = Config.seq_len,
    batch_size: int = Config.batch_size,
    val_split: float = 0.05,
    stride: int = 1,
    shuffle_seed: int = 42,
):
    tokenizer = Tokenizer()
    load_vocab_to_tokenizer(tokenizer, vocab_path)

    token_ids = corpus_to_token_ids(tokenizer, corpus_path)
    if len(token_ids) < seq_len + 1:
        raise ValueError("Corpus too small: need more tokens than seq_len")

    dataset = LMSequenceDataset(token_ids, seq_len=seq_len, stride=stride)

    # split
    total = len(dataset)
    val_count = max(1, int(total * val_split))
    train_count = total - val_count

    # Use deterministic split for reproducibility
    generator = torch.Generator().manual_seed(shuffle_seed)
    train_ds, val_ds = random_split(dataset, [train_count, val_count], generator=generator)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    return tokenizer, train_loader, val_loader

# ---------------------------
# Quick test / example usage
# ---------------------------
if __name__ == "__main__":
    # ensure files exist
    assert os.path.exists(Config.DATA_PATH), f"Corpus not found at {Config.DATA_PATH}"
    assert os.path.exists(Config.VOCAB_PATH), f"Vocab not found at {Config.VOCAB_PATH}"

    tokenizer, train_loader, val_loader = create_dataloaders_from_corpus(
        corpus_path=Config.DATA_PATH,
        vocab_path=Config.VOCAB_PATH,
        seq_len=Config.seq_len,
        batch_size=Config.batch_size,
        val_split=0.05,
        stride=1,
    )

    print("Vocab size:", len(tokenizer.vocab))
    print("Train batches:", len(train_loader))
    print("Val batches:", len(val_loader))

    # show one batch
    for x, y in train_loader:
        print("Input batch shape:", x.shape)
        print("Target batch shape:", y.shape)
        break
