import torch
from src.model.transformer_model import Transformer
from src.data.bpe_tokenizer import BPETokenizer
from src.config.model_config import Config
import os

class Generator:
    def __init__(self, checkpoint_path):
        print(f"Using device: {Config.device}")
        # Load tokenizer
        self.tokenizer = BPETokenizer(
            vocab_path=Config.TOKENIZER_PATH,
            merges_path=Config.MERGES_PATH,
        )
        vocab_size = self.tokenizer.tokenizer.get_vocab_size()

        # Build Transformer
        self.model = Transformer(
            vocab_size=vocab_size,
            embed_dim=Config.embed_dim,
            num_heads=Config.n_heads,
            ff_dim=Config.ffn_dim,
            num_encoder_layers=Config.n_layers,
            num_decoder_layers=Config.n_layers,
            max_len=Config.max_seq_len,
        ).to(Config.device)

        # Load checkpoint
        print(f"üìÇ Loading checkpoint: {checkpoint_path}")
        state_dict = torch.load(checkpoint_path, map_location=Config.device)
        self.model.load_state_dict(state_dict)
        self.model.eval()
        print("‚úÖ Model loaded successfully.\n")
        self.device = Config.device

    def generate(self, prompt, max_tokens=50, temperature=1.0, top_k=50, top_p=0.9):
        # Encode prompt
        input_ids = self.tokenizer.encode(prompt)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(self.device)

        # Decoder starts with <bos>
        bos_id = self.tokenizer.tokenizer.token_to_id("<bos>")
        eos_id = self.tokenizer.tokenizer.token_to_id("<eos>")
        decoder_input = torch.tensor([[bos_id]], dtype=torch.long).to(self.device)

        for _ in range(max_tokens):
            logits = self.model(input_tensor, decoder_input)  # [1, seq_len, vocab]
            next_token_logits = logits[0, -1] / temperature  # apply temperature

            # Top-k filtering
            if top_k > 0:
                topk_vals, topk_idx = torch.topk(next_token_logits, top_k)
                mask = next_token_logits < topk_vals[-1]
                next_token_logits[mask] = -float('Inf')

            # Top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_idx = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)
                sorted_idx_to_remove = cumulative_probs > top_p
                sorted_idx_to_remove[1:] = sorted_idx_to_remove[:-1].clone()
                sorted_idx_to_remove[0] = 0
                indices_to_remove = sorted_idx[sorted_idx_to_remove]
                next_token_logits[indices_to_remove] = -float('Inf')

            # Sample next token
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)

            # Stop if EOS
            if next_token.item() == eos_id:
                break

        # Remove BOS token
        generated_ids = decoder_input[0].tolist()[1:]
        return self.tokenizer.decode(generated_ids)


if __name__ == "__main__":
    generator = Generator("models/checkpoints/model.pt_epoch7.pt")
    prompt = "Hello, how are you?"
    output = generator.generate(prompt, max_tokens=50, temperature=0.8, top_k=50, top_p=0.9)
    print(f"\nüìù Prompt: {prompt}")
    print(f"ü§ñ Generated: {output}")